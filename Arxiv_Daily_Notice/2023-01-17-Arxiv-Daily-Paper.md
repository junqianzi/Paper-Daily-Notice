# New submissions for Mon, 16 Jan 23
## Keyword: SLAM
### A LiDAR-Inertial-Visual SLAM System with Loop Detection
 - **Authors:** Kangcheng Liu
 - **Subjects:** Robotics (cs.RO)
 - **Arxiv link:** https://arxiv.org/abs/2301.05604
 - **Pdf link:** https://arxiv.org/pdf/2301.05604
 - **Abstract**
 We have proposed, to the best of our knowledge, the first-of-its-kind LiDAR-Inertial-Visual-Fused simultaneous localization and mapping (SLAM) system with a strong place recognition capacity. Our proposed SLAM system is consist of visual-inertial odometry (VIO) and LiDAR inertial odometry (LIO) subsystems. We propose the LIO subsystem utilizing the measurement from the LiDAR and the inertial sensors to build the local odometry map, and propose the VIO subsystem which takes in the visual information to construct the 2D-3D associated map. Then, we propose an iterative Kalman Filter-based optimization function to optimize the local project-based 2D-to-3D photo-metric error between the projected image pixels and the local 3D points to make the robust 2D-3D alignment. Finally, we have also proposed the back-end pose graph global optimization and the elaborately designed loop closure detection network to improve the accuracy of the whole SLAM system. Extensive experiments deployed on the UGV in complicated real-world circumstances demonstrate that our proposed LiDAR-Visual-Inertial localization system outperforms the current state-of-the-art in terms of accuracy, efficiency, and robustness.
## Keyword: odometry
### A LiDAR-Inertial-Visual SLAM System with Loop Detection
 - **Authors:** Kangcheng Liu
 - **Subjects:** Robotics (cs.RO)
 - **Arxiv link:** https://arxiv.org/abs/2301.05604
 - **Pdf link:** https://arxiv.org/pdf/2301.05604
 - **Abstract**
 We have proposed, to the best of our knowledge, the first-of-its-kind LiDAR-Inertial-Visual-Fused simultaneous localization and mapping (SLAM) system with a strong place recognition capacity. Our proposed SLAM system is consist of visual-inertial odometry (VIO) and LiDAR inertial odometry (LIO) subsystems. We propose the LIO subsystem utilizing the measurement from the LiDAR and the inertial sensors to build the local odometry map, and propose the VIO subsystem which takes in the visual information to construct the 2D-3D associated map. Then, we propose an iterative Kalman Filter-based optimization function to optimize the local project-based 2D-to-3D photo-metric error between the projected image pixels and the local 3D points to make the robust 2D-3D alignment. Finally, we have also proposed the back-end pose graph global optimization and the elaborately designed loop closure detection network to improve the accuracy of the whole SLAM system. Extensive experiments deployed on the UGV in complicated real-world circumstances demonstrate that our proposed LiDAR-Visual-Inertial localization system outperforms the current state-of-the-art in terms of accuracy, efficiency, and robustness.
## Keyword: livox
There is no result 
## Keyword: loam
There is no result 
## Keyword: lidar
### A LiDAR-Inertial-Visual SLAM System with Loop Detection
 - **Authors:** Kangcheng Liu
 - **Subjects:** Robotics (cs.RO)
 - **Arxiv link:** https://arxiv.org/abs/2301.05604
 - **Pdf link:** https://arxiv.org/pdf/2301.05604
 - **Abstract**
 We have proposed, to the best of our knowledge, the first-of-its-kind LiDAR-Inertial-Visual-Fused simultaneous localization and mapping (SLAM) system with a strong place recognition capacity. Our proposed SLAM system is consist of visual-inertial odometry (VIO) and LiDAR inertial odometry (LIO) subsystems. We propose the LIO subsystem utilizing the measurement from the LiDAR and the inertial sensors to build the local odometry map, and propose the VIO subsystem which takes in the visual information to construct the 2D-3D associated map. Then, we propose an iterative Kalman Filter-based optimization function to optimize the local project-based 2D-to-3D photo-metric error between the projected image pixels and the local 3D points to make the robust 2D-3D alignment. Finally, we have also proposed the back-end pose graph global optimization and the elaborately designed loop closure detection network to improve the accuracy of the whole SLAM system. Extensive experiments deployed on the UGV in complicated real-world circumstances demonstrate that our proposed LiDAR-Visual-Inertial localization system outperforms the current state-of-the-art in terms of accuracy, efficiency, and robustness.
## Keyword: loop detection
There is no result 
## Keyword: nerf
There is no result 
## Keyword: mapping
### Neural Shadow Mapping
 - **Authors:** Sayantan Datta, Derek Nowrouzezahrai, Christoph Schied, Zhao Dong
 - **Subjects:** Graphics (cs.GR)
 - **Arxiv link:** https://arxiv.org/abs/2301.05262
 - **Pdf link:** https://arxiv.org/pdf/2301.05262
 - **Abstract**
 We present a neural extension of basic shadow mapping for fast, high quality hard and soft shadows. We compare favorably to fast pre-filtering shadow mapping, all while producing visual results on par with ray traced hard and soft shadows. We show that combining memory bandwidth-aware architecture specialization and careful temporal-window training leads to a fast, compact and easy-to-train neural shadowing method. Our technique is memory bandwidth conscious, eliminates the need for post-process temporal anti-aliasing or denoising, and supports scenes with dynamic view, emitters and geometry while remaining robust to unseen objects.
### Language-Informed Transfer Learning for Embodied Household Activities
 - **Authors:** Yuqian Jiang, Qiaozi Gao, Govind Thattai, Gaurav Sukhatme
 - **Subjects:** Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
 - **Arxiv link:** https://arxiv.org/abs/2301.05318
 - **Pdf link:** https://arxiv.org/pdf/2301.05318
 - **Abstract**
 For service robots to become general-purpose in everyday household environments, they need not only a large library of primitive skills, but also the ability to quickly learn novel tasks specified by users. Fine-tuning neural networks on a variety of downstream tasks has been successful in many vision and language domains, but research is still limited on transfer learning between diverse long-horizon tasks. We propose that, compared to reinforcement learning for a new household activity from scratch, home robots can benefit from transferring the value and policy networks trained for similar tasks. We evaluate this idea in the BEHAVIOR simulation benchmark which includes a large number of household activities and a set of action primitives. For easy mapping between state spaces of different tasks, we provide a text-based representation and leverage language models to produce a common embedding space. The results show that the selection of similar source activities can be informed by the semantic similarity of state and goal descriptions with the target task. We further analyze the results and discuss ways to overcome the problem of catastrophic forgetting.
### A LiDAR-Inertial-Visual SLAM System with Loop Detection
 - **Authors:** Kangcheng Liu
 - **Subjects:** Robotics (cs.RO)
 - **Arxiv link:** https://arxiv.org/abs/2301.05604
 - **Pdf link:** https://arxiv.org/pdf/2301.05604
 - **Abstract**
 We have proposed, to the best of our knowledge, the first-of-its-kind LiDAR-Inertial-Visual-Fused simultaneous localization and mapping (SLAM) system with a strong place recognition capacity. Our proposed SLAM system is consist of visual-inertial odometry (VIO) and LiDAR inertial odometry (LIO) subsystems. We propose the LIO subsystem utilizing the measurement from the LiDAR and the inertial sensors to build the local odometry map, and propose the VIO subsystem which takes in the visual information to construct the 2D-3D associated map. Then, we propose an iterative Kalman Filter-based optimization function to optimize the local project-based 2D-to-3D photo-metric error between the projected image pixels and the local 3D points to make the robust 2D-3D alignment. Finally, we have also proposed the back-end pose graph global optimization and the elaborately designed loop closure detection network to improve the accuracy of the whole SLAM system. Extensive experiments deployed on the UGV in complicated real-world circumstances demonstrate that our proposed LiDAR-Visual-Inertial localization system outperforms the current state-of-the-art in terms of accuracy, efficiency, and robustness.
## Keyword: localization
### Text to Point Cloud Localization with Relation-Enhanced Transformer
 - **Authors:** Guangzhi Wang, Hehe Fan, Mohan Kankanhalli
 - **Subjects:** Computer Vision and Pattern Recognition (cs.CV)
 - **Arxiv link:** https://arxiv.org/abs/2301.05372
 - **Pdf link:** https://arxiv.org/pdf/2301.05372
 - **Abstract**
 Automatically localizing a position based on a few natural language instructions is essential for future robots to communicate and collaborate with humans. To approach this goal, we focus on the text-to-point-cloud cross-modal localization problem. Given a textual query, it aims to identify the described location from city-scale point clouds. The task involves two challenges. 1) In city-scale point clouds, similar ambient instances may exist in several locations. Searching each location in a huge point cloud with only instances as guidance may lead to less discriminative signals and incorrect results. 2) In textual descriptions, the hints are provided separately. In this case, the relations among those hints are not explicitly described, leading to difficulties of learning relations. To overcome these two challenges, we propose a unified Relation-Enhanced Transformer (RET) to improve representation discriminability for both point cloud and natural language queries. The core of the proposed RET is a novel Relation-enhanced Self-Attention (RSA) mechanism, which explicitly encodes instance (hint)-wise relations for the two modalities. Moreover, we propose a fine-grained cross-modal matching method to further refine the location predictions in a subsequent instance-hint matching stage. Experimental results on the KITTI360Pose dataset demonstrate that our approach surpasses the previous state-of-the-art method by large margin.
### Guaranteed Encapsulation of Targets with Unknown Motion by a Minimalist  Robotic Swarm
 - **Authors:** Himani Sinhmar, Hadas Kress-Gazit
 - **Subjects:** Robotics (cs.RO)
 - **Arxiv link:** https://arxiv.org/abs/2301.05415
 - **Pdf link:** https://arxiv.org/pdf/2301.05415
 - **Abstract**
 We present a decentralized control algorithm for a robotic swarm given the task of encapsulating static and moving targets in a bounded unknown environment. We consider minimalist robots without memory, explicit communication, or localization information. The state-of-the-art approaches generally assume that the robots in the swarm are able to detect the relative position of neighboring robots and targets in order to provide convergence guarantees. In this work, we propose a novel control law for the guaranteed encapsulation of static and moving targets while avoiding all collisions, when the robots do not know the exact relative location of any robot or target in the environment. We make use of the Lyapunov stability theory to prove the convergence of our control algorithm and provide bounds on the ratio between the target and robot speeds. Furthermore, our proposed approach is able to provide stochastic guarantees under the bounds that we determine on task parameters for scenarios where a target moves faster than a robot. Finally, we present an analysis of how the emergent behavior changes with different parameters of the task and noisy sensor readings.
### CLIP the Gap: A Single Domain Generalization Approach for Object  Detection
 - **Authors:** Vidit Vidit, Martin Engilberge, Mathieu Salzmann
 - **Subjects:** Computer Vision and Pattern Recognition (cs.CV)
 - **Arxiv link:** https://arxiv.org/abs/2301.05499
 - **Pdf link:** https://arxiv.org/pdf/2301.05499
 - **Abstract**
 Single Domain Generalization (SDG) tackles the problem of training a model on a single source domain so that it generalizes to any unseen target domain. While this has been well studied for image classification, the literature on SDG object detection remains almost non-existent. To address the challenges of simultaneously learning robust object localization and representation, we propose to leverage a pre-trained vision-language model to introduce semantic domain concepts via textual prompts. We achieve this via a semantic augmentation strategy acting on the features extracted by the detector backbone, as well as a text-based classification loss. Our experiments evidence the benefits of our approach, outperforming by 10% the only existing SDG object detection method, Single-DGOD [49], on their own diverse weather-driving benchmark.
### A LiDAR-Inertial-Visual SLAM System with Loop Detection
 - **Authors:** Kangcheng Liu
 - **Subjects:** Robotics (cs.RO)
 - **Arxiv link:** https://arxiv.org/abs/2301.05604
 - **Pdf link:** https://arxiv.org/pdf/2301.05604
 - **Abstract**
 We have proposed, to the best of our knowledge, the first-of-its-kind LiDAR-Inertial-Visual-Fused simultaneous localization and mapping (SLAM) system with a strong place recognition capacity. Our proposed SLAM system is consist of visual-inertial odometry (VIO) and LiDAR inertial odometry (LIO) subsystems. We propose the LIO subsystem utilizing the measurement from the LiDAR and the inertial sensors to build the local odometry map, and propose the VIO subsystem which takes in the visual information to construct the 2D-3D associated map. Then, we propose an iterative Kalman Filter-based optimization function to optimize the local project-based 2D-to-3D photo-metric error between the projected image pixels and the local 3D points to make the robust 2D-3D alignment. Finally, we have also proposed the back-end pose graph global optimization and the elaborately designed loop closure detection network to improve the accuracy of the whole SLAM system. Extensive experiments deployed on the UGV in complicated real-world circumstances demonstrate that our proposed LiDAR-Visual-Inertial localization system outperforms the current state-of-the-art in terms of accuracy, efficiency, and robustness.
## Keyword: transformer
### Blind Judgement: Agent-Based Supreme Court Modelling With GPT
 - **Authors:** Sil Hamilton
 - **Subjects:** Computation and Language (cs.CL)
 - **Arxiv link:** https://arxiv.org/abs/2301.05327
 - **Pdf link:** https://arxiv.org/pdf/2301.05327
 - **Abstract**
 We present a novel Transformer-based multi-agent system for simulating the judicial rulings of the 2010-2016 Supreme Court of the United States. We train nine separate models with the respective authored opinions of each supreme justice active ca. 2015 and test the resulting system on 96 real-world cases. We find our system predicts the decisions of the real-world Supreme Court with better-than-random accuracy. We further find a correlation between model accuracy with respect to individual justices and their alignment between legal conservatism & liberalism. Our methods and results hold significance for researchers interested in using language models to simulate politically-charged discourse between multiple agents.
### TransfQMix: Transformers for Leveraging the Graph Structure of  Multi-Agent Reinforcement Learning Problems
 - **Authors:** Matteo Gallici, Mario Martin, Ivan Masmitja
 - **Subjects:** Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)
 - **Arxiv link:** https://arxiv.org/abs/2301.05334
 - **Pdf link:** https://arxiv.org/pdf/2301.05334
 - **Abstract**
 Coordination is one of the most difficult aspects of multi-agent reinforcement learning (MARL). One reason is that agents normally choose their actions independently of one another. In order to see coordination strategies emerging from the combination of independent policies, the recent research has focused on the use of a centralized function (CF) that learns each agent's contribution to the team reward. However, the structure in which the environment is presented to the agents and to the CF is typically overlooked. We have observed that the features used to describe the coordination problem can be represented as vertex features of a latent graph structure. Here, we present TransfQMix, a new approach that uses transformers to leverage this latent structure and learn better coordination policies. Our transformer agents perform a graph reasoning over the state of the observable entities. Our transformer Q-mixer learns a monotonic mixing-function from a larger graph that includes the internal and external states of the agents. TransfQMix is designed to be entirely transferable, meaning that same parameters can be used to control and train larger or smaller teams of agents. This enables to deploy promising approaches to save training time and derive general policies in MARL, such as transfer learning, zero-shot transfer, and curriculum learning. We report TransfQMix's performances in the Spread and StarCraft II environments. In both settings, it outperforms state-of-the-art Q-Learning models, and it demonstrates effectiveness in solving problems that other methods can not solve.
### GOHSP: A Unified Framework of Graph and Optimization-based Heterogeneous  Structured Pruning for Vision Transformer
 - **Authors:** Miao Yin, Burak Uzkent, Yilin Shen, Hongxia Jin, Bo Yuan
 - **Subjects:** Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)
 - **Arxiv link:** https://arxiv.org/abs/2301.05345
 - **Pdf link:** https://arxiv.org/pdf/2301.05345
 - **Abstract**
 The recently proposed Vision transformers (ViTs) have shown very impressive empirical performance in various computer vision tasks, and they are viewed as an important type of foundation model. However, ViTs are typically constructed with large-scale sizes, which then severely hinder their potential deployment in many practical resources-constrained applications. To mitigate this challenging problem, structured pruning is a promising solution to compress model size and enable practical efficiency. However, unlike its current popularity for CNNs and RNNs, structured pruning for ViT models is little explored. In this paper, we propose GOHSP, a unified framework of Graph and Optimization-based Structured Pruning for ViT models. We first develop a graph-based ranking for measuring the importance of attention heads, and the extracted importance information is further integrated to an optimization-based procedure to impose the heterogeneous structured sparsity patterns on the ViT models. Experimental results show that our proposed GOHSP demonstrates excellent compression performance. On CIFAR-10 dataset, our approach can bring 40% parameters reduction with no accuracy loss for ViT-Small model. On ImageNet dataset, with 30% and 35% sparsity ratio for DeiT-Tiny and DeiT-Small models, our approach achieves 1.65% and 0.76% accuracy increase over the existing structured pruning methods, respectively.
### Text to Point Cloud Localization with Relation-Enhanced Transformer
 - **Authors:** Guangzhi Wang, Hehe Fan, Mohan Kankanhalli
 - **Subjects:** Computer Vision and Pattern Recognition (cs.CV)
 - **Arxiv link:** https://arxiv.org/abs/2301.05372
 - **Pdf link:** https://arxiv.org/pdf/2301.05372
 - **Abstract**
 Automatically localizing a position based on a few natural language instructions is essential for future robots to communicate and collaborate with humans. To approach this goal, we focus on the text-to-point-cloud cross-modal localization problem. Given a textual query, it aims to identify the described location from city-scale point clouds. The task involves two challenges. 1) In city-scale point clouds, similar ambient instances may exist in several locations. Searching each location in a huge point cloud with only instances as guidance may lead to less discriminative signals and incorrect results. 2) In textual descriptions, the hints are provided separately. In this case, the relations among those hints are not explicitly described, leading to difficulties of learning relations. To overcome these two challenges, we propose a unified Relation-Enhanced Transformer (RET) to improve representation discriminability for both point cloud and natural language queries. The core of the proposed RET is a novel Relation-enhanced Self-Attention (RSA) mechanism, which explicitly encodes instance (hint)-wise relations for the two modalities. Moreover, we propose a fine-grained cross-modal matching method to further refine the location predictions in a subsequent instance-hint matching stage. Experimental results on the KITTI360Pose dataset demonstrate that our approach surpasses the previous state-of-the-art method by large margin.
### It's Just a Matter of Time: Detecting Depression with Time-Enriched  Multimodal Transformers
 - **Authors:** Ana-Maria Bucur, Adrian Cosma, Paolo Rosso, Liviu P. Dinu
 - **Subjects:** Computation and Language (cs.CL)
 - **Arxiv link:** https://arxiv.org/abs/2301.05453
 - **Pdf link:** https://arxiv.org/pdf/2301.05453
 - **Abstract**
 Depression detection from user-generated content on the internet has been a long-lasting topic of interest in the research community, providing valuable screening tools for psychologists. The ubiquitous use of social media platforms lays out the perfect avenue for exploring mental health manifestations in posts and interactions with other users. Current methods for depression detection from social media mainly focus on text processing, and only a few also utilize images posted by users. In this work, we propose a flexible time-enriched multimodal transformer architecture for detecting depression from social media posts, using pretrained models for extracting image and text embeddings. Our model operates directly at the user-level, and we enrich it with the relative time between posts by using time2vec positional embeddings. Moreover, we propose another model variant, which can operate on randomly sampled and unordered sets of posts to be more robust to dataset noise. We show that our method, using EmoBERTa and CLIP embeddings, surpasses other methods on two multimodal datasets, obtaining state-of-the-art results of 0.931 F1 score on a popular multimodal Twitter dataset, and 0.902 F1 score on the only multimodal Reddit dataset.
### Natural Language Processing of Aviation Occurrence Reports for Safety  Management
 - **Authors:** Patrick Jonk, Vincent de Vries, Rombout Wever, Georgios Sidiropoulos, Evangelos Kanoulas
 - **Subjects:** Computation and Language (cs.CL); Machine Learning (cs.LG)
 - **Arxiv link:** https://arxiv.org/abs/2301.05663
 - **Pdf link:** https://arxiv.org/pdf/2301.05663
 - **Abstract**
 Occurrence reporting is a commonly used method in safety management systems to obtain insight in the prevalence of hazards and accident scenarios. In support of safety data analysis, reports are often categorized according to a taxonomy. However, the processing of the reports can require significant effort from safety analysts and a common problem is interrater variability in labeling processes. Also, in some cases, reports are not processed according to a taxonomy, or the taxonomy does not fully cover the contents of the documents. This paper explores various Natural Language Processing (NLP) methods to support the analysis of aviation safety occurrence reports. In particular, the problems studied are the automatic labeling of reports using a classification model, extracting the latent topics in a collection of texts using a topic model and the automatic generation of probable cause texts. Experimental results showed that (i) under the right conditions the labeling of occurrence reports can be effectively automated with a transformer-based classifier, (ii) topic modeling can be useful for finding the topics present in a collection of reports, and (iii) using a summarization model can be a promising direction for generating probable cause texts.
## Keyword: autonomous driving
There is no result 
