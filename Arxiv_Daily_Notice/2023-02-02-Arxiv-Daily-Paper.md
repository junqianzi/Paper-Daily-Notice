# New submissions for Thu,  2 Feb 23
## Keyword: SLAM
There is no result 
## Keyword: odometry
### EMV-LIO: An Efficient Multiple Vision aided LiDAR-Inertial Odometry
 - **Authors:** Bingqi Shen, Yuyin Chen, Fuzhang Han, Shuwei Dai, Rong Xiong, Yue Wang
 - **Subjects:** Robotics (cs.RO)
 - **Arxiv link:** https://arxiv.org/abs/2302.00216
 - **Pdf link:** https://arxiv.org/pdf/2302.00216
 - **Abstract**
 To deal with the degeneration caused by the incomplete constraints of single sensor, multi-sensor fusion strategies especially in LiDAR-vision-inertial fusion area have attracted much interest from both the industry and the research community in recent years. Considering that a monocular camera is vulnerable to the influence of ambient light from a certain direction and fails, which makes the system degrade into a LiDAR-inertial system, multiple cameras are introduced to expand the visual observation so as to improve the accuracy and robustness of the system. Besides, removing LiDAR's noise via range image, setting condition for nearest neighbor search, and replacing kd-Tree with ikd-Tree are also introduced to enhance the efficiency. Based on the above, we propose an Efficient Multiple vision aided LiDAR-inertial odometry system (EMV-LIO), and evaluate its performance on both open datasets and our custom datasets. Experiments show that the algorithm is helpful to improve the accuracy, robustness and efficiency of the whole system compared with LVI-SAM
## Keyword: livox
There is no result 
## Keyword: loam
There is no result 
## Keyword: lidar
### EMV-LIO: An Efficient Multiple Vision aided LiDAR-Inertial Odometry
 - **Authors:** Bingqi Shen, Yuyin Chen, Fuzhang Han, Shuwei Dai, Rong Xiong, Yue Wang
 - **Subjects:** Robotics (cs.RO)
 - **Arxiv link:** https://arxiv.org/abs/2302.00216
 - **Pdf link:** https://arxiv.org/pdf/2302.00216
 - **Abstract**
 To deal with the degeneration caused by the incomplete constraints of single sensor, multi-sensor fusion strategies especially in LiDAR-vision-inertial fusion area have attracted much interest from both the industry and the research community in recent years. Considering that a monocular camera is vulnerable to the influence of ambient light from a certain direction and fails, which makes the system degrade into a LiDAR-inertial system, multiple cameras are introduced to expand the visual observation so as to improve the accuracy and robustness of the system. Besides, removing LiDAR's noise via range image, setting condition for nearest neighbor search, and replacing kd-Tree with ikd-Tree are also introduced to enhance the efficiency. Based on the above, we propose an Efficient Multiple vision aided LiDAR-inertial odometry system (EMV-LIO), and evaluate its performance on both open datasets and our custom datasets. Experiments show that the algorithm is helpful to improve the accuracy, robustness and efficiency of the whole system compared with LVI-SAM
### A Flexible Framework for Virtual Omnidirectional Vision to Improve  Operator Situation Awareness
 - **Authors:** Martin Oehler, Oskar von Stryk
 - **Subjects:** Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)
 - **Arxiv link:** https://arxiv.org/abs/2302.00362
 - **Pdf link:** https://arxiv.org/pdf/2302.00362
 - **Abstract**
 During teleoperation of a mobile robot, providing good operator situation awareness is a major concern as a single mistake can lead to mission failure. Camera streams are widely used for teleoperation but offer limited field-of-view. In this paper, we present a flexible framework for virtual projections to increase situation awareness based on a novel method to fuse multiple cameras mounted anywhere on the robot. Moreover, we propose a complementary approach to improve scene understanding by fusing camera images and geometric 3D Lidar data to obtain a colorized point cloud. The implementation on a compact omnidirectional camera reduces system complexity considerably and solves multiple use-cases on a much smaller footprint compared to traditional approaches such as actuated pan-tilt units. Finally, we demonstrate the generality of the approach by application to the multi-camera system of the Boston Dynamics Spot. The software implementation is available as open-source ROS packages on the project page https://tu-darmstadt-ros-pkg.github.io/omnidirectional_vision.
### Extrinsic Calibration of 2D mm-Wavelength Radar Pairs Using Ego-Velocity  Estimates
 - **Authors:** Qilong Cheng, Emmett Wise, Jonathan Kelly
 - **Subjects:** Robotics (cs.RO); Signal Processing (eess.SP)
 - **Arxiv link:** https://arxiv.org/abs/2302.00660
 - **Pdf link:** https://arxiv.org/pdf/2302.00660
 - **Abstract**
 Correct radar data fusion depends on knowledge of the spatial transform between sensor pairs. Current methods for determining this transform operate by aligning identifiable features in different radar scans, or by relying on measurements from another, more accurate sensor (e.g., a lidar unit). Feature-based alignment requires the sensors to have overlapping fields of view or necessitates the construction of an environment map. Several existing methods require bespoke retroreflective radar targets. These requirements limit both where and how calibration can be performed. In this paper, we take a different approach: instead of attempting to track targets or features, which can be difficult in noisy radar data, we instead rely on ego-velocity estimates from each radar to perform calibration. Our method enables calibration of a subset of the transform parameters, including the yaw and axis of translation between the radar pair, without the need for a shared field of view or for specialized structures in the environment. In general, the yaw and axis of translation are the most important parameters for data fusion, the most likely to vary over time, and the most difficult to calibrate manually. We formulate calibration as a batch optimization problem, prove that the radar-radar system is identifiable, and specify the platform excitation requirements. Through simulations studies and real-world experiments, we establish that our method is more reliable and accurate at estimating the yaw and translation axis than state-of-the-art methods. Finally, we show that the full rigid-body transform can be recovered if relatively coarse information about the rotation rate is available.
## Keyword: loop detection
There is no result 
## Keyword: nerf
There is no result 
## Keyword: mapping
### Probabilistic Point Cloud Modeling via Self-Organizing Gaussian Mixture  Models
 - **Authors:** Kshitij Goel, Nathan Michael, Wennie Tabib
 - **Subjects:** Machine Learning (cs.LG); Graphics (cs.GR); Robotics (cs.RO)
 - **Arxiv link:** https://arxiv.org/abs/2302.00047
 - **Pdf link:** https://arxiv.org/pdf/2302.00047
 - **Abstract**
 This letter presents a continuous probabilistic modeling methodology for spatial point cloud data using finite Gaussian Mixture Models (GMMs) where the number of components are adapted based on the scene complexity. Few hierarchical and adaptive methods have been proposed to address the challenge of balancing model fidelity with size. Instead, state-of-the-art mapping approaches require tuning parameters for specific use cases, but do not generalize across diverse environments. To address this gap, we utilize a self-organizing principle from information-theoretic learning to automatically adapt the complexity of the GMM model based on the relevant information in the sensor data. The approach is evaluated against existing point cloud modeling techniques on real-world data with varying degrees of scene complexity.
### Universal Topological Regularities of Syntactic Structures: Decoupling  Efficiency from Optimization
 - **Authors:** Fermín Moscoso del Prado Martín
 - **Subjects:** Computation and Language (cs.CL); Neurons and Cognition (q-bio.NC)
 - **Arxiv link:** https://arxiv.org/abs/2302.00129
 - **Pdf link:** https://arxiv.org/pdf/2302.00129
 - **Abstract**
 Human syntactic structures are usually represented as graphs. Much research has focused on the mapping between such graphs and linguistic sequences, but less attention has been paid to the shapes of the graphs themselves: their topologies. This study investigates how the topologies of syntactic graphs reveal traces of the processes that led to their emergence. I report a new universal regularity in syntactic structures: Their topology is communicatively efficient above chance. The pattern holds, without exception, for all 124 languages studied, across linguistic families and modalities (spoken, written, and signed). This pattern can arise from a process optimizing for communicative efficiency or, alternatively, by construction, as a by-effect of a sublinear preferential attachment process reflecting language production mechanisms known from psycholinguistics. This dual explanation shows how communicative efficiency, per se, does not require optimization. Among the two options, efficiency without optimization offers the better explanation for the new pattern.
### Fairness-aware Cross-Domain Recommendation
 - **Authors:** Jiakai Tang, Xu Chen, Xueyang Feng
 - **Subjects:** Information Retrieval (cs.IR)
 - **Arxiv link:** https://arxiv.org/abs/2302.00158
 - **Pdf link:** https://arxiv.org/pdf/2302.00158
 - **Abstract**
 Cross-Domain Recommendation (CDR) is an effective way to alleviate the cold-start problem. However, previous work severely ignores fairness and bias when learning the mapping function, which is used to obtain the representations for fresh users in the target domain. To study this problem, in this paper, we propose a Fairness-aware Cross-Domain Recommendation model, called FairCDR. Our method achieves user-oriented group fairness by learning the fairness-aware mapping function. Since the overlapping data are quite limited and distributionally biased, FairCDR leverages abundant non-overlapping users and interactions to help alleviate these problems. Considering that each individual has different influence on model fairness, we propose a new reweighing method based on Influence Function (IF) to reduce unfairness while maintaining recommendation accuracy. Extensive experiments are conducted to demonstrate the effectiveness of our model.
### Zero Shot Transfer of Legal Judgement Prediction as Article-aware  Entailment for the European Court of Human Rights
 - **Authors:** Santosh T.Y.S.S, Oana Ichim, Matthias Grabmair
 - **Subjects:** Computation and Language (cs.CL)
 - **Arxiv link:** https://arxiv.org/abs/2302.00609
 - **Pdf link:** https://arxiv.org/pdf/2302.00609
 - **Abstract**
 In this paper, we cast Legal Judgment Prediction (LJP) from text on European Court of Human Rights cases as an entailment task, where the case outcome is classified from a combined input of case facts and convention articles. This configuration facilitates the model learning legal reasoning ability in mapping article text to specific fact text. It also provides the opportunity to evaluate the model's ability to generalize to zero-shot settings when asked to classify the case outcome with respect to articles not seen during training. We devise zero-shot LJP experiments and apply domain adaptation methods based on domain discriminator and Wasserstein distance. Our results demonstrate that the entailment architecture outperforms straightforward fact classification. We also find that domain adaptation methods improve zero-shot transfer performance, with article relatedness and encoder pre-training influencing the effect.
### Graph Neural Operators for Classification of Spatial Transcriptomics  Data
 - **Authors:** Junaid Ahmed, Alhassan S. Yasin
 - **Subjects:** Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Genomics (q-bio.GN); Quantitative Methods (q-bio.QM)
 - **Arxiv link:** https://arxiv.org/abs/2302.00658
 - **Pdf link:** https://arxiv.org/pdf/2302.00658
 - **Abstract**
 The inception of spatial transcriptomics has allowed improved comprehension of tissue architectures and the disentanglement of complex underlying biological, physiological, and pathological processes through their positional contexts. Recently, these contexts, and by extension the field, have seen much promise and elucidation with the application of graph learning approaches. In particular, neural operators have risen in regards to learning the mapping between infinite-dimensional function spaces. With basic to deep neural network architectures being data-driven, i.e. dependent on quality data for prediction, neural operators provide robustness by offering generalization among different resolutions despite low quality data. Graph neural operators are a variant that utilize graph networks to learn this mapping between function spaces. The aim of this research is to identify robust machine learning architectures that integrate spatial information to predict tissue types. Under this notion, we propose a study incorporating various graph neural network approaches to validate the efficacy of applying neural operators towards prediction of brain regions in mouse brain tissue samples as a proof of concept towards our purpose. We were able to achieve an F1 score of nearly 72% for the graph neural operator approach which outperformed all baseline and other graph network approaches.
## Keyword: localization
### Fast and Noise-Resilient Magnetic Field Mapping on a Low-Cost UAV Using  Gaussian Process Regression
 - **Authors:** Prince E. Kuevor, Maani Ghaffari, Ella M. Atkins, James W. Cutler
 - **Subjects:** Robotics (cs.RO)
 - **Arxiv link:** https://arxiv.org/abs/2302.00113
 - **Pdf link:** https://arxiv.org/pdf/2302.00113
 - **Abstract**
 This work presents a number of techniques to improve the ability to create magnetic field maps on a UAV which can be used to quickly and reliably gather magnetic field observations at multiple altitudes in a workspace. Unfortunately, the electronics on the UAV can introduce their own magnetic fields, distorting the resultant magnetic field map. We show methods of reducing and working with UAV-induced noise to better enable magnetic fields as a sensing modality for indoor navigation. First, some gains in our flight controller create high-frequency motor commands that introduce large noise in the measured magnetic field. Next, we implement a common noise reduction method of distancing the magnetometer from other components on our UAV. Finally, we introduce what we call a compromise GPR (Gaussian process regression) map that can be trained on multiple flight tests to learn any flight-by-flight variations between UAV observation tests. We investigate the spatial density of observations used to train a GPR map then use the compromise map to define a consistency test that can indicate whether or not the magnetometer data and corresponding GPR map are appropriate to use for state estimation. The interventions we introduce in this work facilitate indoor position localization of a UAV whose estimates we found to be quite sensitive to noise generated by the UAV.
### Learning Topology-Preserving Data Representations
 - **Authors:** Ilya Trofimov, Daniil Cherniavskii, Eduard Tulchinskii, Nikita Balabin, Evgeny Burnaev, Serguei Barannikov
 - **Subjects:** Machine Learning (cs.LG); Algebraic Topology (math.AT)
 - **Arxiv link:** https://arxiv.org/abs/2302.00136
 - **Pdf link:** https://arxiv.org/pdf/2302.00136
 - **Abstract**
 We propose a method for learning topology-preserving data representations (dimensionality reduction). The method aims to provide topological similarity between the data manifold and its latent representation via enforcing the similarity in topological features (clusters, loops, 2D voids, etc.) and their localization. The core of the method is the minimization of the Representation Topology Divergence (RTD) between original high-dimensional data and low-dimensional representation in latent space. RTD minimization provides closeness in topological features with strong theoretical guarantees. We develop a scheme for RTD differentiation and apply it as a loss term for the autoencoder. The proposed method ``RTD-AE'' better preserves the global structure and topology of the data manifold than state-of-the-art competitors as measured by linear correlation, triplet distance ranking accuracy, and Wasserstein distance between persistence barcodes.
### Detection of Tomato Ripening Stages using Yolov3-tiny
 - **Authors:** Gerardo Antonio Alvarez Hernández, Juan Carlos Olguin, Juan Irving Vasquez, Abril Valeria Uriarte, Maria Claudia Villicaña Torres
 - **Subjects:** Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)
 - **Arxiv link:** https://arxiv.org/abs/2302.00164
 - **Pdf link:** https://arxiv.org/pdf/2302.00164
 - **Abstract**
 One of the most important agricultural products in Mexico is the tomato (Solanum lycopersicum), which occupies the 4th place national most produced product . Therefore, it is necessary to improve its production, building automatic detection system that detect, classify an keep tacks of the fruits is one way to archieve it. So, in this paper, we address the design of a computer vision system to detect tomatoes at different ripening stages. To solve the problem, we use a neural network-based model for tomato classification and detection. Specifically, we use the YOLOv3-tiny model because it is one of the lightest current deep neural networks. To train it, we perform two grid searches testing several combinations of hyperparameters. Our experiments showed an f1-score of 90.0% in the localization and classification of ripening stages in a custom dataset.
### Learning Generalized Zero-Shot Learners for Open-Domain Image  Geolocalization
 - **Authors:** Lukas Haas, Silas Alberti, Michal Skreta
 - **Subjects:** Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)
 - **Arxiv link:** https://arxiv.org/abs/2302.00275
 - **Pdf link:** https://arxiv.org/pdf/2302.00275
 - **Abstract**
 Image geolocalization is the challenging task of predicting the geographic coordinates of origin for a given photo. It is an unsolved problem relying on the ability to combine visual clues with general knowledge about the world to make accurate predictions across geographies. We present $\href{https://huggingface.co/geolocal/StreetCLIP}{\text{StreetCLIP}}$, a robust, publicly available foundation model not only achieving state-of-the-art performance on multiple open-domain image geolocalization benchmarks but also doing so in a zero-shot setting, outperforming supervised models trained on more than 4 million images. Our method introduces a meta-learning approach for generalized zero-shot learning by pretraining CLIP from synthetic captions, grounding CLIP in a domain of choice. We show that our method effectively transfers CLIP's generalized zero-shot capabilities to the domain of image geolocalization, improving in-domain generalized zero-shot performance without finetuning StreetCLIP on a fixed set of classes.
### K-D Bonsai: ISA-Extensions to Compress K-D Trees for Autonomous Driving  Tasks
 - **Authors:** Pedro H. E. Becker, José María Arnau, Antonio González
 - **Subjects:** Hardware Architecture (cs.AR)
 - **Arxiv link:** https://arxiv.org/abs/2302.00361
 - **Pdf link:** https://arxiv.org/pdf/2302.00361
 - **Abstract**
 Autonomous Driving (AD) systems extensively manipulate 3D point clouds for object detection and vehicle localization. Thereby, efficient processing of 3D point clouds is crucial in these systems. In this work we propose K-D Bonsai, a technique to cut down memory usage during radius search, a critical building block of point cloud processing. K-D Bonsai exploits value similarity in the data structure that holds the point cloud (a k-d tree) to compress the data in memory. K-D Bonsai further compresses the data using a reduced floating-point representation, exploiting the physically limited range of point cloud values. For easy integration into nowadays systems, we implement K-D Bonsai through Bonsai-extensions, a small set of new CPU instructions to compress, decompress, and operate on points. To maintain baseline safety levels, we carefully craft the Bonsai-extensions to detect precision loss due to compression, allowing re-computation in full precision to take place if necessary. Therefore, K-D Bonsai reduces data movement, improving performance and energy efficiency, while guaranteeing baseline accuracy and programmability. We evaluate K-D Bonsai over the euclidean cluster task of Autoware.ai, a state-of-the-art software stack for AD. We achieve an average of 9.26% improvement in end-to-end latency, 12.19% in tail latency, and a reduction of 10.48% in energy consumption. Differently from expensive accelerators proposed in related work, K-D Bonsai improves radius search with minimal area increase (0.36%).
### An automated, geometry-based method for the analysis of hippocampal  thickness
 - **Authors:** Kersten Diers, Hannah Baumeister, Frank Jessen, Emrah Düzel, David Berron, Martin Reuter
 - **Subjects:** Computer Vision and Pattern Recognition (cs.CV); Computational Geometry (cs.CG)
 - **Arxiv link:** https://arxiv.org/abs/2302.00573
 - **Pdf link:** https://arxiv.org/pdf/2302.00573
 - **Abstract**
 The hippocampus is one of the most studied neuroanatomical structures due to its involvement in attention, learning, and memory as well as its atrophy in ageing, neurological, and psychiatric diseases. Hippocampal shape changes, however, are complex and cannot be fully characterized by a single summary metric such as hippocampal volume as determined from MR images. In this work, we propose an automated, geometry-based approach for the unfolding, point-wise correspondence, and local analysis of hippocampal shape features such as thickness and curvature. Starting from an automated segmentation of hippocampal subfields, we create a 3D tetrahedral mesh model as well as a 3D intrinsic coordinate system of the hippocampal body. From this coordinate system, we derive local curvature and thickness estimates as well as a 2D sheet for hippocampal unfolding. We evaluate the performance of our algorithm with a series of experiments to quantify neurodegenerative changes in Mild Cognitive Impairment and Alzheimer's disease dementia. We find that hippocampal thickness estimates detect known differences between clinical groups and can determine the location of these effects on the hippocampal sheet. Further, thickness estimates improve classification of clinical groups and cognitively unimpaired controls when added as an additional predictor. Comparable results are obtained with different datasets and segmentation algorithms. Taken together, we replicate canonical findings on hippocampal volume/shape changes in dementia, extend them by gaining insight into their spatial localization on the hippocampal sheet, and provide additional, complementary information beyond traditional measures. We provide a new set of sensitive processing and analysis tools for the analysis of hippocampal geometry that allows comparisons across studies without relying on image registration or requiring manual intervention.
### Autonomous Local Catalog Maintenance of Close Proximity Satellite  Systems on Closed Natural Motion Trajectories
 - **Authors:** Christopher W. Hays, Kristina Miller, Alexander Soderlund, Sean Phillips, Troy Henderson
 - **Subjects:** Multiagent Systems (cs.MA); Systems and Control (eess.SY)
 - **Arxiv link:** https://arxiv.org/abs/2302.00601
 - **Pdf link:** https://arxiv.org/pdf/2302.00601
 - **Abstract**
 To enable space mission sets like on-orbit servicing and manufacturing, agents in close proximity maybe operating too close to yield resolved localization solutions to operators from ground sensors. This leads to a requirement on the systems need to maintain a catalog of their local neighborhood, however, this may impose a large burden on each agent by requiring updating and maintenance of this catalog at each node. To alleviate this burden, this paper considers the case of a single satellite agent (a chief) updating a single catalog. More specifically, we consider the case of numerous satellite deputy agents in a local neighborhood of a chief, the goal of the chief satellite is to maintain and update a catalog of all agents within this neighborhood through onboard measurements. We consider the agents having relative translational and attitude motion dynamics between the chief and deputy, with the chief centered at the origin of the frame. We provide an end-to-end solution of the this problem through providing both a supervisory control method coupled with a Bayesian Filter that propagates the belief state and provides the catalog solutions to the supervisor. The goal of the supervisory controller is to determine which agent to look at and at which times while adhering to constraints of the chief satellite. We provide a numerical validation to this problem with three agents.
## Keyword: transformer
### Transformers Meet Directed Graphs
 - **Authors:** Simon Geisler, Yujia Li, Daniel Mankowitz, Ali Taylan Cemgil, Stephan Günnemann, Cosmin Paduraru
 - **Subjects:** Machine Learning (cs.LG)
 - **Arxiv link:** https://arxiv.org/abs/2302.00049
 - **Pdf link:** https://arxiv.org/pdf/2302.00049
 - **Abstract**
 Transformers were originally proposed as a sequence-to-sequence model for text but have become vital for a wide range of modalities, including images, audio, video, and undirected graphs. However, transformers for directed graphs are a surprisingly underexplored topic, despite their applicability to ubiquitous domains including source code and logic circuits. In this work, we propose two direction- and structure-aware positional encodings for directed graphs: (1) the eigenvectors of the Magnetic Laplacian - a direction-aware generalization of the combinatorial Laplacian; (2) directional random walk encodings. Empirically, we show that the extra directionality information is useful in various downstream tasks, including correctness testing of sorting networks and source code understanding. Together with a data-flow-centric graph construction, our model outperforms the prior state of the art on the Open Graph Benchmark Code2 relatively by 14.7%.
### Real Estate Property Valuation using Self-Supervised Vision Transformers
 - **Authors:** Mahdieh Yazdani, Maziar Raissi
 - **Subjects:** Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Econometrics (econ.EM)
 - **Arxiv link:** https://arxiv.org/abs/2302.00117
 - **Pdf link:** https://arxiv.org/pdf/2302.00117
 - **Abstract**
 The use of Artificial Intelligence (AI) in the real estate market has been growing in recent years. In this paper, we propose a new method for property valuation that utilizes self-supervised vision transformers, a recent breakthrough in computer vision and deep learning. Our proposed algorithm uses a combination of machine learning, computer vision and hedonic pricing models trained on real estate data to estimate the value of a given property. We collected and pre-processed a data set of real estate properties in the city of Boulder, Colorado and used it to train, validate and test our algorithm. Our data set consisted of qualitative images (including house interiors, exteriors, and street views) as well as quantitative features such as the number of bedrooms, bathrooms, square footage, lot square footage, property age, crime rates, and proximity to amenities. We evaluated the performance of our model using metrics such as Root Mean Squared Error (RMSE). Our findings indicate that these techniques are able to accurately predict the value of properties, with a low RMSE. The proposed algorithm outperforms traditional appraisal methods that do not leverage property images and has the potential to be used in real-world applications.
### Efficient Scopeformer: Towards Scalable and Rich Feature Extraction for  Intracranial Hemorrhage Detection
 - **Authors:** Yassine Barhoumi, Nidhal C. Bouaynaya, Ghulam Rasool
 - **Subjects:** Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)
 - **Arxiv link:** https://arxiv.org/abs/2302.00220
 - **Pdf link:** https://arxiv.org/pdf/2302.00220
 - **Abstract**
 The quality and richness of feature maps extracted by convolution neural networks (CNNs) and vision Transformers (ViTs) directly relate to the robust model performance. In medical computer vision, these information-rich features are crucial for detecting rare cases within large datasets. This work presents the "Scopeformer," a novel multi-CNN-ViT model for intracranial hemorrhage classification in computed tomography (CT) images. The Scopeformer architecture is scalable and modular, which allows utilizing various CNN architectures as the backbone with diversified output features and pre-training strategies. We propose effective feature projection methods to reduce redundancies among CNN-generated features and to control the input size of ViTs. Extensive experiments with various Scopeformer models show that the model performance is proportional to the number of convolutional blocks employed in the feature extractor. Using multiple strategies, including diversifying the pre-training paradigms for CNNs, different pre-training datasets, and style transfer techniques, we demonstrate an overall improvement in the model performance at various computational budgets. Later, we propose smaller compute-efficient Scopeformer versions with three different types of input and output ViT configurations. Efficient Scopeformers use four different pre-trained CNN architectures as feature extractors to increase feature richness. Our best Efficient Scopeformer model achieved an accuracy of 96.94\% and a weighted logarithmic loss of 0.083 with an eight times reduction in the number of trainable parameters compared to the base Scopeformer. Another version of the Efficient Scopeformer model further reduced the parameter space by almost 17 times with negligible performance reduction. Hybrid CNNs and ViTs might provide the desired feature richness for developing accurate medical computer vision models
### Multispectral Pedestrian Detection via Reference Box Constrained Cross  Attention and Modality Balanced Optimization
 - **Authors:** Yinghui Xing, Song Wang, Guoqiang Liang, Qingyi Li, Xiuwei Zhang, Shizhou Zhang, Yanning Zhang
 - **Subjects:** Computer Vision and Pattern Recognition (cs.CV)
 - **Arxiv link:** https://arxiv.org/abs/2302.00290
 - **Pdf link:** https://arxiv.org/pdf/2302.00290
 - **Abstract**
 Multispectral pedestrian detection is an important task for many around-the-clock applications, since the visible and thermal modalities can provide complementary information especially under low light conditions. To reduce the influence of hand-designed components in available multispectral pedestrian detectors, we propose a MultiSpectral pedestrian DEtection TRansformer (MS-DETR), which extends deformable DETR to multi-modal paradigm. In order to facilitate the multi-modal learning process, a Reference box Constrained Cross-Attention (RCCA) module is firstly introduced to the multi-modal Transformer decoder, which takes fusion branch together with the reference boxes as intermediaries to enable the interaction of visible and thermal modalities. To further balance the contribution of different modalities, we design a modality-balanced optimization strategy, which aligns the slots of decoders by adaptively adjusting the instance-level weight of three branches. Our end-to-end MS-DETR shows superior performance on the challenging KAIST and CVC-14 benchmark datasets.
### The geometry of hidden representations of large transformer models
 - **Authors:** Lucrezia Valeriani, Diego Doimo, Francesca Cuturello, Alessandro Laio, Alessio Ansuini, Alberto Cazzaniga
 - **Subjects:** Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
 - **Arxiv link:** https://arxiv.org/abs/2302.00294
 - **Pdf link:** https://arxiv.org/pdf/2302.00294
 - **Abstract**
 Large transformers are powerful architectures for self-supervised analysis of data of various nature, ranging from protein sequences to text to images. In these models, the data representation in the hidden layers live in the same space, and the semantic structure of the dataset emerges by a sequence of functionally identical transformations between one representation and the next. We here characterize the geometric and statistical properties of these representations, focusing on the evolution of such proprieties across the layers. By analyzing geometric properties such as the intrinsic dimension (ID) and the neighbor composition we find that the representations evolve in a strikingly similar manner in transformers trained on protein language tasks and image reconstruction tasks. In the first layers, the data manifold expands, becoming high-dimensional, and then it contracts significantly in the intermediate layers. In the last part of the model, the ID remains approximately constant or forms a second shallow peak. We show that the semantic complexity of the dataset emerges at the end of the first peak. This phenomenon can be observed across many models trained on diverse datasets. Based on these observations, we suggest using the ID profile as an unsupervised proxy to identify the layers which are more suitable for downstream learning tasks.
### An Evaluation of Persian-English Machine Translation Datasets with  Transformers
 - **Authors:** Amir Sartipi, Meghdad Dehghan, Afsaneh Fatemi
 - **Subjects:** Computation and Language (cs.CL); Artificial Intelligence (cs.AI)
 - **Arxiv link:** https://arxiv.org/abs/2302.00321
 - **Pdf link:** https://arxiv.org/pdf/2302.00321
 - **Abstract**
 Nowadays, many researchers are focusing their attention on the subject of machine translation (MT). However, Persian machine translation has remained unexplored despite a vast amount of research being conducted in languages with high resources, such as English. Moreover, while a substantial amount of research has been undertaken in statistical machine translation for some datasets in Persian, there is currently no standard baseline for transformer-based text2text models on each corpus. This study collected and analysed the most popular and valuable parallel corpora, which were used for Persian-English translation. Furthermore, we fine-tuned and evaluated two state-of-the-art attention-based seq2seq models on each dataset separately (48 results). We hope this paper will assist researchers in comparing their Persian to English and vice versa machine translation results to a standard baseline.
### Attention Link: An Efficient Attention-Based Low Resource Machine  Translation Architecture
 - **Authors:** Zeping Min
 - **Subjects:** Computation and Language (cs.CL); Artificial Intelligence (cs.AI)
 - **Arxiv link:** https://arxiv.org/abs/2302.00340
 - **Pdf link:** https://arxiv.org/pdf/2302.00340
 - **Abstract**
 Transformers have achieved great success in machine translation, but transformer-based NMT models often require millions of bilingual parallel corpus for training. In this paper, we propose a novel architecture named as attention link (AL) to help improve transformer models' performance, especially in low training resources. We theoretically demonstrate the superiority of our attention link architecture in low training resources. Besides, we have done a large number of experiments, including en-de, de-en, en-fr, en-it, it-en, en-ro translation tasks on the IWSLT14 dataset as well as real low resources scene on bn-gu and gu-ta translation tasks on the CVIT PIB dataset. All the experiment results show our attention link is powerful and can lead to a significant improvement. In addition, we achieve a 37.9 BLEU score, a new sota, on the IWSLT14 de-en task by combining our attention link and other advanced methods.
### An Empirical Study on the Transferability of Transformer Modules in  Parameter-Efficient Fine-Tuning
 - **Authors:** Mohammad AkbarTajari, Sara Rajaee, Mohammad Taher Pilehvar
 - **Subjects:** Computation and Language (cs.CL)
 - **Arxiv link:** https://arxiv.org/abs/2302.00378
 - **Pdf link:** https://arxiv.org/pdf/2302.00378
 - **Abstract**
 Parameter-efficient fine-tuning approaches have recently garnered a lot of attention. Having considerably lower number of trainable weights, these methods can bring about scalability and computational effectiveness. In this paper, we look for optimal sub-networks and investigate the capability of different transformer modules in transferring knowledge from a pre-trained model to a downstream task. Our empirical results suggest that every transformer module in BERT can act as a winning ticket: fine-tuning each specific module while keeping the rest of the network frozen can lead to comparable performance to the full fine-tuning. Among different modules, LayerNorms exhibit the best capacity for knowledge transfer with limited trainable weights, to the extent that, with only 0.003% of all parameters in the layer-wise analysis, they show acceptable performance on various target tasks. On the reasons behind their effectiveness, we argue that their notable performance could be attributed to their high-magnitude weights compared to that of the other modules in the pre-trained BERT.
### Multimodality Representation Learning: A Survey on Evolution,  Pretraining and Its Applications
 - **Authors:** Muhammad Arslan Manzoor, Sarah Albarri, Ziting Xian, Zaiqiao Meng, Preslav Nakov, Shangsong Liang
 - **Subjects:** Artificial Intelligence (cs.AI)
 - **Arxiv link:** https://arxiv.org/abs/2302.00389
 - **Pdf link:** https://arxiv.org/pdf/2302.00389
 - **Abstract**
 Multimodality Representation Learning, as a technique of learning to embed information from different modalities and their correlations, has achieved remarkable success on a variety of applications, such as Visual Question Answering (VQA), Natural Language for Visual Reasoning (NLVR), and Vision Language Retrieval (VLR). Among these applications, cross-modal interaction and complementary information from different modalities are crucial for advanced models to perform any multimodal task, e.g., understand, recognize, retrieve, or generate optimally. Researchers have proposed diverse methods to address these tasks. The different variants of transformer-based architectures performed extraordinarily on multiple modalities. This survey presents the comprehensive literature on the evolution and enhancement of deep learning multimodal architectures to deal with textual, visual and audio features for diverse cross-modal and modern multimodal tasks. This study summarizes the (i) recent task-specific deep learning methodologies, (ii) the pretraining types and multimodal pretraining objectives, (iii) from state-of-the-art pretrained multimodal approaches to unifying architectures, and (iv) multimodal task categories and possible future improvements that can be devised for better multimodal learning. Moreover, we prepare a dataset section for new researchers that covers most of the benchmarks for pretraining and finetuning. Finally, major challenges, gaps, and potential research topics are explored. A constantly-updated paperlist related to our survey is maintained at https://github.com/marslanm/multimodality-representation-learning.
### Hierarchical Classification of Research Fields in the "Web of Science"  Using Deep Learning
 - **Authors:** Susie Xi Rao, Peter H. Egger, Ce Zhang
 - **Subjects:** Digital Libraries (cs.DL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
 - **Arxiv link:** https://arxiv.org/abs/2302.00390
 - **Pdf link:** https://arxiv.org/pdf/2302.00390
 - **Abstract**
 The scholarly publication space is growing steadily not just in numbers but also in complexity due to collaboration between individuals from within and across fields of research. This paper presents a hierarchical classification system that automatically categorizes a scholarly publication using its abstract into a three-tier hierarchical label set of fields (discipline-field-subfield). This system enables a holistic view about the interdependence of research activities in the mentioned hierarchical tiers in terms of knowledge production through articles and impact through citations. The classification system (44 disciplines - 738 fields - 1,501 subfields) utilizes and is able to cope with 160 million abstract snippets in Microsoft Academic Graph (Version 2018-05-17) using batch training in a modularized and distributed fashion to address and assess interdisciplinarity and inter-field classifications. In addition, we have explored multi-class classifications in both the single-label and multi-label settings. In total, we have conducted 3,140 experiments, in all models (Convolutional Neural Networks, Recurrent Neural Networks, Transformers), the classification accuracy is > 90% in 77.84% and 78.83% of the single-label and multi-label classifications, respectively. We examine the advantages of our classification by its ability to better align research texts and output with disciplines, to adequately classify them in an automated way, as well as to capture the degree of interdisciplinarity in a publication which enables downstream analytics such as field interdisciplinarity. This system (a set of pretrained models) can serve as a backbone to an interactive system of indexing scientific publications.
### Do I Have Your Attention: A Large Scale Engagement Prediction Dataset  and Baselines
 - **Authors:** Monisha Singh, Ximi Hoque, Donghuo Zeng, Yanan Wang, Kazushi Ikeda, Abhinav Dhall
 - **Subjects:** Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)
 - **Arxiv link:** https://arxiv.org/abs/2302.00431
 - **Pdf link:** https://arxiv.org/pdf/2302.00431
 - **Abstract**
 The degree of concentration, enthusiasm, optimism, and passion displayed by individual(s) while interacting with a machine is referred to as `user engagement'. Engagement comprises of behavioural, cognitive, and affect related cues. To create engagement predictions systems, which can work in real-world conditions it is quintessential to learn from rich diverse datasets. To this end, a large scale multi-faceted engagement in the wild dataset is proposed. 31 hours duration data of 127 participants representing different illumination conditions is recorded. Thorough experiments are performed exploring applicability of different features action units, eye gaze and head pose and transformers. To further validate the rich nature of the dataset, evaluation is also performed on the EngageWild dataset. The experiments show the usefulness of the proposed dataset. The code, models and dataset will be made publicly available.
### Feed-Forward Blocks Control Contextualization in Masked Language Models
 - **Authors:** Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, Kentaro Inui
 - **Subjects:** Computation and Language (cs.CL)
 - **Arxiv link:** https://arxiv.org/abs/2302.00456
 - **Pdf link:** https://arxiv.org/pdf/2302.00456
 - **Abstract**
 Understanding the inner workings of neural network models is a crucial step for rationalizing their output and refining their architecture. Transformer-based models are the core of recent natural language processing and have been analyzed typically with attention patterns as their epoch-making feature is contextualizing surrounding input words via attention mechanisms. In this study, we analyze their inner contextualization by considering all the components, including the feed-forward block (i.e., a feed-forward layer and its surrounding residual and normalization layers) as well as the attention. Our experiments with masked language models show that each of the previously overlooked components did modify the degree of the contextualization in case of processing special word-word pairs (e.g., consisting of named entities). Furthermore, we find that some components cancel each other's effects. Our results could update the typical view about each component's roles (e.g., attention performs contextualization, and the other components serve different roles) in the Transformer layer.
### Clinical Decision Transformer: Intended Treatment Recommendation through  Goal Prompting
 - **Authors:** Seunghyun Lee, Da Young Lee, Sujeong Im, Nan Hee Kim, Sung-Min Park
 - **Subjects:** Artificial Intelligence (cs.AI)
 - **Arxiv link:** https://arxiv.org/abs/2302.00612
 - **Pdf link:** https://arxiv.org/pdf/2302.00612
 - **Abstract**
 With recent achievements in tasks requiring context awareness, foundation models have been adopted to treat large-scale data from electronic health record (EHR) systems. However, previous clinical recommender systems based on foundation models have a limited purpose of imitating clinicians' behavior and do not directly consider a problem of missing values. In this paper, we propose Clinical Decision Transformer (CDT), a recommender system that generates a sequence of medications to reach a desired range of clinical states given as goal prompts. For this, we conducted goal-conditioned sequencing, which generated a subsequence of treatment history with prepended future goal state, and trained the CDT to model sequential medications required to reach that goal state. For contextual embedding over intra-admission and inter-admissions, we adopted a GPT-based architecture with an admission-wise attention mask and column embedding. In an experiment, we extracted a diabetes dataset from an EHR system, which contained treatment histories of 4788 patients. We observed that the CDT achieved the intended treatment effect according to goal prompt ranges (e.g., NormalA1c, LowerA1c, and HigherA1c), contrary to the case with behavior cloning. To the best of our knowledge, this is the first study to explore clinical recommendations from the perspective of goal prompting. See https://clinical-decision-transformer.github.io for code and additional information.
### ADAPT: Action-aware Driving Caption Transformer
 - **Authors:** Bu Jin, Xinyu Liu, Yupeng Zheng, Pengfei Li, Hao Zhao, Tong Zhang, Yuhang Zheng, Guyue Zhou, Jingjing Liu
 - **Subjects:** Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)
 - **Arxiv link:** https://arxiv.org/abs/2302.00673
 - **Pdf link:** https://arxiv.org/pdf/2302.00673
 - **Abstract**
 End-to-end autonomous driving has great potential in the transportation industry. However, the lack of transparency and interpretability of the automatic decision-making process hinders its industrial adoption in practice. There have been some early attempts to use attention maps or cost volume for better model explainability which is difficult for ordinary passengers to understand. To bridge the gap, we propose an end-to-end transformer-based architecture, ADAPT (Action-aware Driving cAPtion Transformer), which provides user-friendly natural language narrations and reasoning for each decision making step of autonomous vehicular control and action. ADAPT jointly trains both the driving caption task and the vehicular control prediction task, through a shared video representation. Experiments on BDD-X (Berkeley DeepDrive eXplanation) dataset demonstrate state-of-the-art performance of the ADAPT framework on both automatic metrics and human evaluation. To illustrate the feasibility of the proposed framework in real-world applications, we build a novel deployable system that takes raw car videos as input and outputs the action narrations and reasoning in real time. The code, models and data are available at https://github.com/jxbbb/ADAPT.
## Keyword: autonomous driving
### Development of Real-time Rendering Technology for High-Precision Models  in Autonomous Driving
 - **Authors:** Zhang Whencheng, Wang Chengyi
 - **Subjects:** Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)
 - **Arxiv link:** https://arxiv.org/abs/2302.00291
 - **Pdf link:** https://arxiv.org/pdf/2302.00291
 - **Abstract**
 Our autonomous driving simulation lab produces a high-precision 3D model simulating the parking lot. However, the current model still has poor rendering quality in some aspects. In this work, we develop a system to improve the rendering of the model and evaluate the quality of the rendered model.
### K-D Bonsai: ISA-Extensions to Compress K-D Trees for Autonomous Driving  Tasks
 - **Authors:** Pedro H. E. Becker, José María Arnau, Antonio González
 - **Subjects:** Hardware Architecture (cs.AR)
 - **Arxiv link:** https://arxiv.org/abs/2302.00361
 - **Pdf link:** https://arxiv.org/pdf/2302.00361
 - **Abstract**
 Autonomous Driving (AD) systems extensively manipulate 3D point clouds for object detection and vehicle localization. Thereby, efficient processing of 3D point clouds is crucial in these systems. In this work we propose K-D Bonsai, a technique to cut down memory usage during radius search, a critical building block of point cloud processing. K-D Bonsai exploits value similarity in the data structure that holds the point cloud (a k-d tree) to compress the data in memory. K-D Bonsai further compresses the data using a reduced floating-point representation, exploiting the physically limited range of point cloud values. For easy integration into nowadays systems, we implement K-D Bonsai through Bonsai-extensions, a small set of new CPU instructions to compress, decompress, and operate on points. To maintain baseline safety levels, we carefully craft the Bonsai-extensions to detect precision loss due to compression, allowing re-computation in full precision to take place if necessary. Therefore, K-D Bonsai reduces data movement, improving performance and energy efficiency, while guaranteeing baseline accuracy and programmability. We evaluate K-D Bonsai over the euclidean cluster task of Autoware.ai, a state-of-the-art software stack for AD. We achieve an average of 9.26% improvement in end-to-end latency, 12.19% in tail latency, and a reduction of 10.48% in energy consumption. Differently from expensive accelerators proposed in related work, K-D Bonsai improves radius search with minimal area increase (0.36%).
### ADAPT: Action-aware Driving Caption Transformer
 - **Authors:** Bu Jin, Xinyu Liu, Yupeng Zheng, Pengfei Li, Hao Zhao, Tong Zhang, Yuhang Zheng, Guyue Zhou, Jingjing Liu
 - **Subjects:** Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)
 - **Arxiv link:** https://arxiv.org/abs/2302.00673
 - **Pdf link:** https://arxiv.org/pdf/2302.00673
 - **Abstract**
 End-to-end autonomous driving has great potential in the transportation industry. However, the lack of transparency and interpretability of the automatic decision-making process hinders its industrial adoption in practice. There have been some early attempts to use attention maps or cost volume for better model explainability which is difficult for ordinary passengers to understand. To bridge the gap, we propose an end-to-end transformer-based architecture, ADAPT (Action-aware Driving cAPtion Transformer), which provides user-friendly natural language narrations and reasoning for each decision making step of autonomous vehicular control and action. ADAPT jointly trains both the driving caption task and the vehicular control prediction task, through a shared video representation. Experiments on BDD-X (Berkeley DeepDrive eXplanation) dataset demonstrate state-of-the-art performance of the ADAPT framework on both automatic metrics and human evaluation. To illustrate the feasibility of the proposed framework in real-world applications, we build a novel deployable system that takes raw car videos as input and outputs the action narrations and reasoning in real time. The code, models and data are available at https://github.com/jxbbb/ADAPT.
